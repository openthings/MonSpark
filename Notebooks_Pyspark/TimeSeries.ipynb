from pyspark.sql import SQLContext
from pyspark.sql.functions import *
from pyspark.sql.functions import from_unixtime, unix_timestamp
from pyspark.sql.types import TimestampType

from datetime import date, timedelta
from os import path
sqlContext = SQLContext(sc)

from pyspark.sql.types import DateType, StructType, StructField, FloatType, StringType, DecimalType
from pyspark.sql.functions import to_date

from pyspark.mllib.linalg import Vectors
import matplotlib.pyplot as plt

%pylab inline

from pyspark.sql.types import *
from pyspark.sql import functions as F

lst = [('a', 'b1', 12.0), ('a', 'b1', 10.0),
       ('a', 'b2', 15.5),
        ('a2', 'b1', 23.3)
      ]
schema = StructType([
    StructField("col1", StringType(), False),
    StructField("col2", StringType(), False),    
    StructField("value", FloatType(), False)])
#Create a dataframe Ã  partir de cette liste 
df = sc.parallelize(lst).toDF(schema)


df2 = df.groupBy(df.col1, df.col2).agg(F.avg(df.value), F.max(df.value))
df3 = df2.withColumnRenamed("avg(value)", "avg_value").withColumnRenamed(
    "max(value)", "max_value")

df3.show()


#Data Preparatio
weights = [.8, .1, .1]
seed = 42
parsedTrainData, parsedValData, parsedTestData = parsedData.randomSplit(weights, seed)

parsedTrainData.cache()
#parsedValData.cache()
#parsedTestData.cache()
nTrain = parsedTrainData.count()
#nVal = parsedValData.count()
#nTest = parsedTestData.count()

print nTrain#, nVal, nTest, nTrain + nVal + nTest
print parsedData.count()

#=========
from pyspark.ml.regression import LinearRegression
lr3 = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
# Fit the model
model3 = lr3.fit(parsedTrainData.toDF())

predictions = model3.transform(parsedData.toDF())
#predictions.show()
# Evaluate the model on training data
#valuesAndPreds = predictions.map(lambda p: (p.label, model3.predict(p.features)))
MSE = predictions.map(lambda (feat, lab, pred): (lab - pred)**2).reduce(
    lambda x, y: x + y) / predictions.count()
print("Mean Squared Error = " + str(MSE))

#============
# Load and parse the data
from pyspark.sql import SQLContext
from pyspark.mllib.regression import LabeledPoint

def parsePoint(tup):
    return LabeledPoint(tup[1], [tup[0]]) 

lst = [(1.0, 12.40), (2.0, 12.47),
       (3,12.51), (4, 12.54), (5, 8.80), (6, 11.50), (7, 10.30), (8, 6.70), (9, 5.50), (10, 4.30)]

data = sc.parallelize(lst)

parsedData = data.map(parsePoint)
parsedData.collect()

#=================

%pylab inline
import pandas as pd
import numpy as np
from pandas import Series, DataFrame, Panel


from pyspark.sql.types import DateType,StructType, StructField, StringType
from pyspark.sql.functions import to_date

schema_df_toPlot = StructType([
    StructField("label", StringType(), False),
    StructField("value", StringType(), False)])

def plotRdd(rdd_lp):
    "rdd : is an rdd of a labled points [(label, value)]. label and value are float"
    #df_transformed = rdd.map(lambda (x, y) : (int(x), int(y))).toDF(schema_df_toPlot)
    #TODO : transforme la df en un panda dataframe
    x1 = rdd_lp.map(lambda (x, y) : x).collect()
    y1 = rdd_lp.map(lambda (x, y) : y).collect()
    plt.scatter(x1, y1, color = 'green', linewidths=0.1)
    #plt.show()
    
def plotPredictedValues(df_with_pred):
    "df : a dataFrame of the forme : (features,label,prediction)"
    x2 = df_with_pred.map(lambda (f, l, p) : int(f[0])).collect()
    y2 = df_with_pred.map(lambda (f, l, p) : int(p)).collect()
    plt.scatter(x2, y2, color = 'red', linewidths=0.1)

#==================
plotRdd(data)
plotPredictedValues(predictions)
plt.show()
#=========


import pyspark.sql.functions as F

def addSumAvgToNimsoftDF(nimsoftDf):
    "nimsoftDf : a data frame with schema ()"
    #res = imsoftDf.groupeBy("host", "role", "appli", "metric").agg(F.avg(nimsoftDf.value), F.max(nimsoftDf.value)
         #                                ).withColumnRenamed(
        #"avg(value)", "avg_value").withColumnRenamed("max(value)", "max_value")
        
    res = nimsoftDf.groupBy("metric").agg(F.avg(nimsoftDf.value), F.max(nimsoftDf.value)
                                         ).withColumnRenamed(
        "avg(value)", "avg_value").withColumnRenamed("max(value)", "max_value")
    return res;

#Test
lst2 = [("A", 12.40), ("A", 12.47), ("b", 12.51), ("c", 12.54)]
schema2 = StructType([
    StructField("metric", StringType(), False),
    StructField("value", StringType(), False)])
df2 = sc.parallelize(lst2).toDF(schema2)
df3 = addSumAvgNimsoftDF(df2)
df3.show()
