from pyspark.sql import SQLContext
from pyspark.sql.functions import *
from pyspark.sql.functions import from_unixtime, unix_timestamp
from pyspark.sql.types import TimestampType

from datetime import date, timedelta
from os import path
sqlContext = SQLContext(sc)

from pyspark.sql.types import DateType, StructType, StructField, FloatType, StringType, DecimalType
from pyspark.sql.functions import to_date

from pyspark.mllib.linalg import Vectors
import matplotlib.pyplot as plt

%pylab inline

from pyspark.sql.types import *
from pyspark.sql import functions as F

lst = [('a', 'b1', 12.0), ('a', 'b1', 10.0),
       ('a', 'b2', 15.5),
        ('a2', 'b1', 23.3)
      ]
schema = StructType([
    StructField("col1", StringType(), False),
    StructField("col2", StringType(), False),    
    StructField("value", FloatType(), False)])
#Create a dataframe Ã  partir de cette liste 
df = sc.parallelize(lst).toDF(schema)


df2 = df.groupBy(df.col1, df.col2).agg(F.avg(df.value), F.max(df.value))
df3 = df2.withColumnRenamed("avg(value)", "avg_value").withColumnRenamed(
    "max(value)", "max_value")

df3.show()


#Data Preparatio
weights = [.8, .1, .1]
seed = 42
parsedTrainData, parsedValData, parsedTestData = parsedData.randomSplit(weights, seed)

parsedTrainData.cache()
#parsedValData.cache()
#parsedTestData.cache()
nTrain = parsedTrainData.count()
#nVal = parsedValData.count()
#nTest = parsedTestData.count()

print nTrain#, nVal, nTest, nTrain + nVal + nTest
print parsedData.count()

